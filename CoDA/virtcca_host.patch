From eeeae85b01ca57c30999f8b61b0ba6e11d7cfc0b Mon Sep 17 00:00:00 2001
From: yangxiangkai <yangxiangkai@huawei.com>
Date: Tue, 10 Sep 2024 15:06:14 +0800
Subject: [PATCH] virtcca feature: vfio driver adapt to coda

virtcca inclusion
category: feature
bugzilla: https://gitee.com/openeuler/kernel/issues/IAQON6

--------------------------------

VirtCCA Coda Feature:
vfio driver adapt to coda

Signed-off-by: Xiangkai Yang <yangxiangkai@huawei.com>
Signed-off-by: Junbin Li <lijunbin4@huawei.com>
---
---
 arch/arm64/include/asm/kvm_tmi.h              |   8 +-
 arch/arm64/include/asm/kvm_tmm.h              |  26 +-
 arch/arm64/include/asm/virtcca_coda.h         |  66 +++
 arch/arm64/kernel/Makefile                    |   2 +-
 arch/arm64/kernel/virtcca_coda.c              | 531 ++++++++++++++++++
 arch/arm64/kvm/mmu.c                          |   8 +-
 arch/arm64/kvm/tmi.c                          |  40 ++
 arch/arm64/kvm/virtcca_cvm.c                  | 312 +++++++++-
 drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.c |  41 +-
 drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.h |   4 +-
 drivers/iommu/dma-iommu.c                     |  50 ++
 drivers/iommu/io-pgtable-arm.c                |  24 +
 drivers/iommu/iommu.c                         |  75 +++
 drivers/pci/access.c                          |  19 +
 drivers/pci/msi/msi.c                         |  20 +
 drivers/pci/msi/msi.h                         |  16 +
 drivers/vfio/group.c                          |  31 +
 drivers/vfio/pci/vfio_pci_core.c              |   7 +
 drivers/vfio/pci/vfio_pci_rdwr.c              |  59 ++
 drivers/vfio/vfio_iommu_type1.c               | 260 +++++++++
 include/linux/pci.h                           |   4 +
 include/linux/vfio.h                          |   5 +-
 include/uapi/linux/vfio.h                     |   2 +
 virt/kvm/vfio.c                               | 226 ++++++++
 24 files changed, 1826 insertions(+), 10 deletions(-)
 create mode 100644 arch/arm64/include/asm/virtcca_coda.h
 create mode 100644 arch/arm64/kernel/virtcca_coda.c

diff --git a/arch/arm64/include/asm/kvm_tmi.h b/arch/arm64/include/asm/kvm_tmi.h
index 36fceec57490..eb2a351a3edb 100644
--- a/arch/arm64/include/asm/kvm_tmi.h
+++ b/arch/arm64/include/asm/kvm_tmi.h
@@ -247,6 +247,7 @@ struct tmi_tec_run {
 #define TMI_FNUM_SMMU_WRITE             U(0x282)
 #define TMI_FNUM_SMMU_READ              U(0x283)
 #define TMI_FNUM_SMMU_PCIE_CORE_CHECK   U(0x284)
+#define TMI_FNUM_DEV_TTT_CREATE         U(0x285)
 
 /* TMI SMC64 PIDs handled by the SPMD */
 #define TMI_TMM_VERSION_REQ             TMI_FID(SMC_64, TMI_FNUM_VERSION_REQ)
@@ -280,6 +281,7 @@ struct tmi_tec_run {
 #define TMI_TMM_SMMU_WRITE              TMI_FID(SMC_64, TMI_FNUM_SMMU_WRITE)
 #define TMI_TMM_SMMU_READ               TMI_FID(SMC_64, TMI_FNUM_SMMU_READ)
 #define TMI_TMM_SMMU_PCIE_CORE_CHECK    TMI_FID(SMC_64, TMI_FNUM_SMMU_PCIE_CORE_CHECK)
+#define TMI_TMM_DEV_TTT_CREATE          TMI_FID(SMC_64, TMI_FNUM_DEV_TTT_CREATE)
 
 #define TMI_ABI_VERSION_GET_MAJOR(_version) ((_version) >> 16)
 #define TMI_ABI_VERSION_GET_MINOR(_version) ((_version) & 0xFFFF)
@@ -381,6 +383,7 @@ u64 tmi_ttt_map_range(u64 rd, u64 map_addr, u64 size, u64 cur_node, u64 target_n
 u64 tmi_ttt_unmap_range(u64 rd, u64 map_addr, u64 size, u64 node_id);
 u64 tmi_mem_info_show(u64 mem_info_addr);
 
+u64 tmi_dev_ttt_create(u64 numa_set, u64 rd, u64 map_addr, u64 level);
 u64 tmi_smmu_queue_create(u64 params_ptr);
 u64 tmi_smmu_queue_write(uint64_t cmd0, uint64_t cmd1, u64 smmu_id);
 u64 tmi_smmu_ste_create(u64 params_ptr);
@@ -396,6 +399,7 @@ u64 tmi_smmu_pcie_core_check(u64 smmu_base);
 u64 tmi_smmu_write(u64 smmu_base, u64 reg_offset, u64 val, u64 bits);
 u64 tmi_smmu_read(u64 smmu_base, u64 reg_offset, u64 bits);
 
+u64 mmio_va_to_pa(void *addr);
 void kvm_cvm_vcpu_put(struct kvm_vcpu *vcpu);
 int kvm_load_user_data(struct kvm *kvm, unsigned long arg);
 unsigned long cvm_psci_vcpu_affinity_info(struct kvm_vcpu *vcpu,
@@ -404,6 +408,8 @@ int kvm_cvm_vcpu_set_events(struct kvm_vcpu *vcpu,
 	bool serror_pending, bool ext_dabt_pending);
 int kvm_init_cvm_vm(struct kvm *kvm);
 int kvm_enable_virtcca_cvm(struct kvm *kvm);
-
+int kvm_cvm_map_ipa(struct kvm *kvm, phys_addr_t ipa, kvm_pfn_t pfn,
+	unsigned long map_size, enum kvm_pgtable_prot prot, int ret);
+void virtcca_cvm_set_secure_flag(void *vdev, void *info);
 #endif
 #endif
diff --git a/arch/arm64/include/asm/kvm_tmm.h b/arch/arm64/include/asm/kvm_tmm.h
index ac1cb415919a..c9dad66c86cd 100644
--- a/arch/arm64/include/asm/kvm_tmm.h
+++ b/arch/arm64/include/asm/kvm_tmm.h
@@ -10,9 +10,21 @@
 /*
  * There is a conflict with the internal iova of CVM,
  * so it is necessary to offset the msi iova.
+ * According to qemu file(hw/arm/virt.c), 0x0a001000 - 0x0b000000
+ * iova is not being used, so it is used as the iova range for msi
+ * mapping.
  */
-#define CVM_MSI_ORIG_IOVA 0x8000000
-#define CVM_MSI_IOVA_OFFSET (-0x1000000)
+#define CVM_MSI_ORIG_IOVA	0x8000000
+#define CVM_MSI_MIN_IOVA	0x0a001000
+#define CVM_MSI_MAX_IOVA	0x0b000000
+#define CVM_MSI_IOVA_OFFSET	0x1000
+
+#define CVM_RW_8_BIT	0x8
+#define CVM_RW_16_BIT	0x10
+#define CVM_RW_32_BIT	0x20
+#define CVM_RW_64_BIT	0x40
+
+#define BUS_NUM_SHIFT	0x8
 
 enum virtcca_cvm_state {
 	CVM_STATE_NONE = 1,
@@ -100,9 +112,14 @@ int kvm_cvm_map_range(struct kvm *kvm);
 int cvm_arm_smmu_domain_set_kvm(void *group);
 int kvm_cvm_map_unmap_ipa_range(struct kvm *kvm, phys_addr_t ipa_base, phys_addr_t pa,
 	unsigned long map_size, uint32_t is_map);
+int cvm_map_unmap_ipa_range(struct kvm *kvm, phys_addr_t ipa_base, phys_addr_t pa,
+	unsigned long map_size, uint32_t is_map);
 int kvm_cvm_map_ipa_mmio(struct kvm *kvm, phys_addr_t ipa_base,
 	phys_addr_t pa, unsigned long map_size);
 
+bool is_in_virtcca_ram_range(struct kvm *kvm, uint64_t iova);
+bool is_virtcca_iova_need_vfio_dma(struct kvm *kvm, uint64_t iova);
+
 #define CVM_TTT_BLOCK_LEVEL	2
 #define CVM_TTT_MAX_LEVEL	3
 
@@ -116,6 +133,11 @@ int kvm_cvm_map_ipa_mmio(struct kvm *kvm, phys_addr_t ipa_base,
 	((CVM_PAGE_SHIFT - 3) * (4 - (l)) + 3)
 #define CVM_L2_BLOCK_SIZE	BIT(CVM_TTT_LEVEL_SHIFT(2))
 
+#define TMM_GRANULE_SIZE2		12
+#define TMM_TTT_WIDTH			19
+#define TMM_GRANULE_SIZE		(1UL << TMM_GRANULE_SIZE2)
+#define tmm_granule_size(level)	(TMM_GRANULE_SIZE << ((3 - level)) * TMM_TTT_WIDTH)
+
 static inline unsigned long cvm_ttt_level_mapsize(int level)
 {
 	if (WARN_ON(level > CVM_TTT_BLOCK_LEVEL))
diff --git a/arch/arm64/include/asm/virtcca_coda.h b/arch/arm64/include/asm/virtcca_coda.h
new file mode 100644
index 000000000000..09fb43fca40c
--- /dev/null
+++ b/arch/arm64/include/asm/virtcca_coda.h
@@ -0,0 +1,66 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright (C) 2024. Huawei Technologies Co., Ltd. All rights reserved.
+ */
+#ifndef __VIRTCCA_CODA_H
+#define __VIRTCCA_CODA_H
+
+#include <linux/iommu.h>
+#include <linux/vfio_pci_core.h>
+
+#include "../../../drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.h"
+#include "../../../drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.h"
+
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+int virtcca_smmu_secure_dev_operator(struct iommu_domain *domain, struct device *dev);
+int virtcca_attach_secure_dev(struct iommu_domain *domain, struct iommu_group *group);
+
+struct iommu_domain *virtcca_iommu_group_get_domain(struct iommu_group *iommu_group);
+u64 virtcca_get_iommu_device_msi_addr(struct iommu_group *iommu_group);
+void virtcca_set_dev_msi_addr(struct iommu_group *iommu_group, unsigned long iova);
+int kvm_get_iommu_group_by_domain(struct kvm *kvm, struct arm_smmu_domain *smmu_domain, phys_addr_t pa,
+	unsigned long map_size);
+
+int virtcca_iommu_map(struct iommu_domain *domain, unsigned long iova,
+	phys_addr_t paddr, size_t size, int prot);
+size_t virtcca_iommu_unmap(struct iommu_domain *domain,
+	unsigned long iova, size_t size);
+int virtcca_map_pages(void *ops, unsigned long iova,
+	phys_addr_t paddr, size_t pgsize, size_t pgcount,
+	int iommu_prot, size_t *mapped);
+size_t virtcca_unmap_pages(void *ops, unsigned long iova,
+	size_t pgsize, size_t pgcount);
+
+void virtcca_pci_read_msi_msg(struct pci_dev *dev, struct msi_msg *msg,
+	void __iomem *base);
+int virtcca_pci_write_msg_msi(struct msi_desc *desc, struct msi_msg *msg);
+void virtcca_msix_prepare_msi_desc(struct pci_dev *dev,
+	struct msi_desc *desc, void __iomem *addr);
+int virtcca_pci_msix_write_vector_ctrl(struct msi_desc *desc, u32 ctrl);
+int virtcca_pci_msix_mask(struct msi_desc *desc);
+int msix_mask_all_cc(struct pci_dev *dev, void __iomem *base, int tsize, u64 dev_num);
+
+int virtcca_pci_generic_config_read(void __iomem *addr, unsigned char bus_num,
+	unsigned int devfn, int size, u32 *val);
+int virtcca_pci_generic_config_write(void __iomem *addr, unsigned char bus_num,
+	unsigned int devfn, int size, u32 val);
+
+bool is_virtcca_pci_io_rw(struct vfio_pci_core_device *vdev);
+void virtcca_pci_io_write(struct vfio_pci_core_device *vdev, u64 val,
+	u64 size, void __iomem *io);
+u64 virtcca_pci_io_read(struct vfio_pci_core_device *vdev,
+	u64 size, void __iomem *io);
+
+bool virtcca_iommu_domain_get_kvm(struct iommu_domain *domain, struct kvm **kvm);
+bool virtcca_check_kvm_is_cvm(void *iommu, struct kvm **kvm);
+int virtcca_vfio_iommu_map(void *iommu, dma_addr_t iova,
+	unsigned long pfn, long npage, int prot);
+int cvm_vfio_add_kvm_to_smmu_domain(struct file *filp, void *kv);
+struct kvm *virtcca_arm_smmu_get_kvm(struct arm_smmu_domain *domain);
+void kvm_get_arm_smmu_domain(struct kvm *kvm, struct list_head *smmu_domain_group_list);
+struct arm_lpae_io_pgtable *virtcca_io_pgtable_get_data(void *ops);
+struct io_pgtable_cfg *virtcca_io_pgtable_get_cfg(struct arm_lpae_io_pgtable *data);
+struct iommu_group *cvm_vfio_file_iommu_group(struct file *file);
+void *virtcca_io_pgtable_get_smmu_domain(struct arm_lpae_io_pgtable *data);
+#endif
+#endif
diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
index 4ce58887302a..fe6334bbe93e 100644
--- a/arch/arm64/kernel/Makefile
+++ b/arch/arm64/kernel/Makefile
@@ -82,7 +82,7 @@ obj-$(CONFIG_ARM64_ILP32)		+= vdso-ilp32/
 obj-$(CONFIG_UNWIND_PATCH_PAC_INTO_SCS)	+= patch-scs.o
 obj-$(CONFIG_IPI_AS_NMI)		+= ipi_nmi.o
 obj-$(CONFIG_HISI_VIRTCCA_GUEST)	+= virtcca_cvm_guest.o virtcca_cvm_tsi.o
-obj-$(CONFIG_HISI_VIRTCCA_HOST)		+= virtcca_cvm_host.o
+obj-$(CONFIG_HISI_VIRTCCA_HOST)		+= virtcca_cvm_host.o virtcca_coda.o
 CFLAGS_patch-scs.o			+= -mbranch-protection=none
 
 # Force dependency (vdso*-wrap.S includes vdso.so through incbin)
diff --git a/arch/arm64/kernel/virtcca_coda.c b/arch/arm64/kernel/virtcca_coda.c
new file mode 100644
index 000000000000..cacdfc18b18d
--- /dev/null
+++ b/arch/arm64/kernel/virtcca_coda.c
@@ -0,0 +1,531 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024. Huawei Technologies Co., Ltd. All rights reserved.
+ */
+#include <linux/pci.h>
+#include <linux/msi.h>
+#include <linux/vfio.h>
+#include <linux/io-pgtable.h>
+#include <asm/virtcca_coda.h>
+#include <asm/virtcca_cvm_host.h>
+#include <asm/kvm_host.h>
+#include <asm/kvm_tmm.h>
+#include <asm/kvm_tmi.h>
+
+#include "../../drivers/pci/msi/msi.h"
+#include "../../drivers/vfio/vfio.h"
+
+/**
+ * virtcca_map_pages - Virtcca need map the secure
+ * memory with paddr
+ * @ops: The handle of io_pgtable_ops
+ * @iova: Ipa address
+ * @paddr: Physical address
+ * @pgsize: Page size
+ * @pgcount: Page count
+ * @iommu_prot: Iommu attribute
+ * @mapped: Mapped size
+ *
+ * Returns:
+ * %0 if map pages success
+ */
+int virtcca_map_pages(void *ops, unsigned long iova,
+	phys_addr_t paddr, size_t pgsize, size_t pgcount,
+	int iommu_prot, size_t *mapped)
+{
+	struct kvm *kvm;
+	u64 loader_start;
+	u64 ram_size;
+	struct arm_lpae_io_pgtable *data = virtcca_io_pgtable_get_data(ops);
+	struct io_pgtable_cfg *cfg = virtcca_io_pgtable_get_cfg(data);
+	long iaext = (s64)iova >> cfg->ias;
+	int ret = 0;
+	struct arm_smmu_domain *smmu_domain = NULL;
+
+	if (WARN_ON(!pgsize || (pgsize & cfg->pgsize_bitmap) != pgsize))
+		return -EINVAL;
+
+	if (cfg->quirks & IO_PGTABLE_QUIRK_ARM_TTBR1)
+		iaext = ~iaext;
+	if (WARN_ON(iaext || paddr >> cfg->oas))
+		return -ERANGE;
+
+	/* If no access, then nothing to do */
+	if (!(iommu_prot & (IOMMU_READ | IOMMU_WRITE)))
+		return 0;
+
+	smmu_domain = (struct arm_smmu_domain *)virtcca_io_pgtable_get_smmu_domain(data);
+	if (!smmu_domain)
+		return -EINVAL;
+
+	kvm = smmu_domain->kvm;
+	if (kvm) {
+		struct virtcca_cvm *virtcca_cvm = kvm->arch.virtcca_cvm;
+
+		loader_start = virtcca_cvm->loader_start;
+		ram_size = virtcca_cvm->ram_size;
+		if (iova >= loader_start &&
+			iova < loader_start + ram_size &&
+			!virtcca_cvm->is_mapped) {
+			ret = kvm_cvm_map_range(kvm);
+		} else if (iova < loader_start) {
+			if (iova == CVM_MSI_ORIG_IOVA)
+				ret = kvm_get_iommu_group_by_domain(kvm, smmu_domain,
+					paddr, pgsize * pgcount);
+			else
+				ret = cvm_map_unmap_ipa_range(kvm, iova,
+					paddr, pgsize * pgcount, true);
+		}
+		if (mapped)
+			*mapped += pgsize * pgcount;
+	}
+	return ret;
+}
+EXPORT_SYMBOL_GPL(virtcca_map_pages);
+
+/**
+ * virtcca_unmap_pages - Virtcca unmap the iova
+ * @ops: The handle of io_pgtable_ops
+ * @iova: Ipa address
+ * @pgsize: Page size
+ * @pgcount: Page count
+ *
+ * Returns:
+ * %0 if map pages success or parameter is invalid
+ */
+size_t virtcca_unmap_pages(void *ops, unsigned long iova,
+	size_t pgsize, size_t pgcount)
+{
+	struct kvm *kvm;
+	struct arm_lpae_io_pgtable *data = virtcca_io_pgtable_get_data(ops);
+	struct io_pgtable_cfg *cfg = virtcca_io_pgtable_get_cfg(data);
+	long iaext = (s64)iova >> cfg->ias;
+	struct arm_smmu_domain *smmu_domain = NULL;
+
+	if (WARN_ON(!pgsize || (pgsize & cfg->pgsize_bitmap) != pgsize || !pgcount))
+		return 0;
+
+	if (cfg->quirks & IO_PGTABLE_QUIRK_ARM_TTBR1)
+		iaext = ~iaext;
+	if (WARN_ON(iaext))
+		return 0;
+
+	smmu_domain = (struct arm_smmu_domain *)virtcca_io_pgtable_get_smmu_domain(data);
+	if (!smmu_domain)
+		return 0;
+
+	kvm = smmu_domain->kvm;
+	if (!kvm)
+		return 0;
+
+	return cvm_map_unmap_ipa_range(kvm, iova, 0, pgsize * pgcount, false);
+}
+EXPORT_SYMBOL_GPL(virtcca_unmap_pages);
+
+/**
+ * virtcca_iommu_map - Iommu driver map pages, and then
+ * calls the map function in the
+ * smmu to perform mapping
+ * @domain: Iommu domain
+ * @iova: Ipa address
+ * @paddr: Physical address
+ * @size: Map size
+ * @prot: Iommu attribute
+ *
+ * Returns:
+ * %0 if map success
+ * %-EINVAL if domain type is not paging
+ * %-ENODEV if the domain pgsize_bitmap is zero or parameter is invalid
+ */
+int virtcca_iommu_map(struct iommu_domain *domain, unsigned long iova,
+	phys_addr_t paddr, size_t size, int prot)
+{
+	unsigned int min_pagesz;
+	int ret = 0;
+	unsigned long orig_iova = iova;
+	size_t orig_size = size;
+	const struct iommu_domain_ops *ops = domain->ops;
+	struct io_pgtable_ops *io_ops = to_smmu_domain(domain)->pgtbl_ops;
+
+	if (unlikely(!(domain->type & __IOMMU_DOMAIN_PAGING)))
+		return -EINVAL;
+
+	if (WARN_ON(domain->pgsize_bitmap == 0UL))
+		return -ENODEV;
+
+	/* find out the minimum page size supported */
+	min_pagesz = 1 << __ffs(domain->pgsize_bitmap);
+
+	/*
+	 * both the virtual address and the physical one, as well as
+	 * the size of the mapping, must be aligned (at least) to the
+	 * size of the smallest page supported by the hardware
+	 */
+	if (!IS_ALIGNED(iova | paddr | size, min_pagesz)) {
+		pr_err("unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
+			iova, &paddr, size, min_pagesz);
+		return -EINVAL;
+	}
+
+	if (!io_ops)
+		return -ENODEV;
+
+	while (size) {
+		size_t pgsize, count, mapped = 0;
+
+		pgsize = iommu_pgsize(domain, iova, paddr, size, &count);
+
+		ret = virtcca_map_pages(io_ops, iova, paddr, pgsize,
+			count, prot, &mapped);
+		/*
+		 * Some pages may have been mapped, even if an error occurred,
+		 * so we should account for those so they can be unmapped.
+		 */
+		size -= mapped;
+
+		if (ret)
+			break;
+
+		iova += mapped;
+		paddr += mapped;
+	}
+
+	/* unroll mapping in case something went wrong */
+	if (ret)
+		virtcca_iommu_unmap(domain, orig_iova, orig_size - size);
+
+	if (ret == 0 && ops->iotlb_sync_map) {
+		ret = ops->iotlb_sync_map(domain, iova, size);
+		if (ret)
+			goto out_err;
+	}
+
+	return ret;
+
+out_err:
+	/* undo mappings already done */
+	virtcca_iommu_unmap(domain, iova, size);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(virtcca_iommu_map);
+
+/**
+ * virtcca_iommu_unmap - Iommu driver unmap pages, and then
+ * calls the map function in the
+ * smmu to perform unmapping
+ * @domain: Iommu domain
+ * @iova: Ipa address
+ * @size: Map size
+ *
+ * Returns:
+ * %0 if map success or domain type and parameter is invalid
+ */
+
+size_t virtcca_iommu_unmap(struct iommu_domain *domain,
+	unsigned long iova, size_t size)
+{
+	size_t unmapped_page, unmapped = 0;
+	struct io_pgtable_ops *ops = to_smmu_domain(domain)->pgtbl_ops;
+	unsigned int min_pagesz;
+
+	if (unlikely(!(domain->type & __IOMMU_DOMAIN_PAGING)))
+		return 0;
+
+	if (WARN_ON(domain->pgsize_bitmap == 0UL))
+		return 0;
+
+	/* find out the minimum page size supported */
+	min_pagesz = 1 << __ffs(domain->pgsize_bitmap);
+
+	/*
+	 * The virtual address, as well as the size of the mapping, must be
+	 * aligned (at least) to the size of the smallest page supported
+	 * by the hardware
+	 */
+	if (!IS_ALIGNED(iova | size, min_pagesz)) {
+		pr_err("unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%x\n",
+			iova, size, min_pagesz);
+		return 0;
+	}
+
+	if (!ops)
+		return 0;
+
+	/*
+	 * Keep iterating until we either unmap 'size' bytes (or more)
+	 * or we hit an area that isn't mapped.
+	 */
+	while (unmapped < size) {
+		size_t pgsize, count;
+
+		pgsize = iommu_pgsize(domain, iova, iova, size - unmapped, &count);
+		unmapped_page = virtcca_unmap_pages(ops, iova, pgsize, count);
+		if (!unmapped_page)
+			break;
+
+		iova += unmapped_page;
+		unmapped += unmapped_page;
+	}
+	return unmapped;
+}
+EXPORT_SYMBOL_GPL(virtcca_iommu_unmap);
+
+/**
+ * virtcca_pci_read_msi_msg - secure dev read msi msg
+ * @dev: Pointer to the pci_dev data structure of MSI-X device function
+ * @msg: Msg information
+ * @base: Msi base address
+ *
+ **/
+void virtcca_pci_read_msi_msg(struct pci_dev *dev, struct msi_msg *msg,
+	void __iomem *base)
+{
+	u64 pbase = mmio_va_to_pa(base);
+
+	msg->address_lo = tmi_mmio_read(pbase + PCI_MSIX_ENTRY_LOWER_ADDR,
+		CVM_RW_32_BIT, pci_dev_id(dev));
+	msg->address_hi = tmi_mmio_read(pbase + PCI_MSIX_ENTRY_UPPER_ADDR,
+		CVM_RW_32_BIT, pci_dev_id(dev));
+	msg->data = tmi_mmio_read(pbase + PCI_MSIX_ENTRY_DATA, CVM_RW_32_BIT, pci_dev_id(dev));
+}
+
+/**
+ * virtcca_pci_write_msi_msg - secure dev write msi msg
+ * @desc: MSI-X description
+ * @msg: Msg information
+ *
+ **/
+int virtcca_pci_write_msg_msi(struct msi_desc *desc, struct msi_msg *msg)
+{
+	if (!is_virtcca_cvm_enable())
+		return 0;
+
+	void __iomem *base = pci_msix_desc_addr(desc);
+	u32 ctrl = desc->pci.msix_ctrl;
+	bool unmasked = !(ctrl & PCI_MSIX_ENTRY_CTRL_MASKBIT);
+	u64 pbase = mmio_va_to_pa(base);
+	struct pci_dev *pdev = (desc->dev != NULL &&
+		dev_is_pci(desc->dev)) ? to_pci_dev(desc->dev) : NULL;
+
+	if (!is_cc_dev(pci_dev_id(pdev)))
+		return 0;
+
+	u64 addr = (u64)msg->address_lo | ((u64)msg->address_hi << 32);
+
+	if (addr) {
+		/* Get the offset of the its register of a specific device */
+		u64 offset = addr - CVM_MSI_ORIG_IOVA;
+
+		addr = get_g_cc_dev_msi_addr(pci_dev_id(pdev));
+		addr += offset;
+		if (!addr)
+			return 1;
+	}
+	tmi_mmio_write(pbase + PCI_MSIX_ENTRY_LOWER_ADDR,
+		lower_32_bits(addr), CVM_RW_32_BIT, pci_dev_id(pdev));
+	tmi_mmio_write(pbase + PCI_MSIX_ENTRY_UPPER_ADDR,
+		upper_32_bits(addr), CVM_RW_32_BIT, pci_dev_id(pdev));
+	tmi_mmio_write(pbase + PCI_MSIX_ENTRY_DATA,
+		msg->data, CVM_RW_32_BIT, pci_dev_id(pdev));
+
+	if (unmasked)
+		pci_msix_write_vector_ctrl(desc, ctrl);
+	tmi_mmio_read(mmio_va_to_pa((void *)pbase + PCI_MSIX_ENTRY_DATA),
+		CVM_RW_32_BIT, pci_dev_id(pdev));
+
+	return 1;
+}
+
+void virtcca_msix_prepare_msi_desc(struct pci_dev *dev,
+	struct msi_desc *desc, void __iomem *addr)
+{
+	desc->pci.msix_ctrl = tmi_mmio_read(mmio_va_to_pa(addr + PCI_MSIX_ENTRY_VECTOR_CTRL),
+		CVM_RW_32_BIT, pci_dev_id(dev));
+}
+
+/*
+ * If it is a safety device, write vector ctrl need
+ * use tmi interface
+ */
+int virtcca_pci_msix_write_vector_ctrl(struct msi_desc *desc, u32 ctrl)
+{
+	if (!is_virtcca_cvm_enable())
+		return 0;
+
+	void __iomem *desc_addr = pci_msix_desc_addr(desc);
+	struct pci_dev *pdev = (desc->dev != NULL &&
+		dev_is_pci(desc->dev)) ? to_pci_dev(desc->dev) : NULL;
+
+	if (pdev == NULL || !is_cc_dev(pci_dev_id(pdev)))
+		return 0;
+
+	if (desc->pci.msi_attrib.can_mask)
+		tmi_mmio_write(mmio_va_to_pa(desc_addr + PCI_MSIX_ENTRY_VECTOR_CTRL),
+			ctrl, CVM_RW_32_BIT, pci_dev_id(pdev));
+	return 1;
+}
+
+/*
+ * If it is a safety device, read msix need
+ * use tmi interface
+ */
+int virtcca_pci_msix_mask(struct msi_desc *desc)
+{
+	if (!is_virtcca_cvm_enable())
+		return 0;
+
+	struct pci_dev *pdev = (desc->dev != NULL &&
+		dev_is_pci(desc->dev)) ? to_pci_dev(desc->dev) : NULL;
+
+	if (pdev == NULL || !is_cc_dev(pci_dev_id(pdev)))
+		return 0;
+
+	/* Flush write to device */
+	tmi_mmio_read(mmio_va_to_pa(desc->pci.mask_base), CVM_RW_32_BIT, pci_dev_id(pdev));
+	return 1;
+}
+
+/**
+ * msix_mask_all_cc - mask all secure dev msix c
+ * @dev: Pointer to the pci_dev data structure of MSI-X device function
+ * @base: Io address
+ * @tsize: Number of entry
+ * @dev_num: Dev number
+ *
+ * Returns:
+ * %0 if msix mask all cc device success
+ **/
+int msix_mask_all_cc(struct pci_dev *dev, void __iomem *base, int tsize, u64 dev_num)
+{
+	int i;
+	u16 rw_ctrl;
+	u32 ctrl = PCI_MSIX_ENTRY_CTRL_MASKBIT;
+	u64 pbase = mmio_va_to_pa(base);
+
+	if (pci_msi_ignore_mask)
+		goto out;
+
+	for (i = 0; i < tsize; i++, base += PCI_MSIX_ENTRY_SIZE) {
+		tmi_mmio_write(pbase + PCI_MSIX_ENTRY_VECTOR_CTRL,
+			ctrl, CVM_RW_32_BIT, dev_num);
+	}
+
+out:
+	pci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, &rw_ctrl);
+	rw_ctrl &= ~PCI_MSIX_FLAGS_MASKALL;
+	rw_ctrl |= 0;
+	pci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, rw_ctrl);
+
+	pcibios_free_irq(dev);
+	return 0;
+}
+
+/* If device is secure dev, read config need transfer to tmm module */
+int virtcca_pci_generic_config_read(void __iomem *addr, unsigned char bus_num,
+	unsigned int devfn, int size, u32 *val)
+{
+	if (size == 1)
+		*val = tmi_mmio_read(mmio_va_to_pa(addr), CVM_RW_8_BIT,
+			((bus_num << BUS_NUM_SHIFT) | devfn));
+	else if (size == 2)
+		*val = tmi_mmio_read(mmio_va_to_pa(addr), CVM_RW_16_BIT,
+			((bus_num << BUS_NUM_SHIFT) | devfn));
+	else
+		*val = tmi_mmio_read(mmio_va_to_pa(addr), CVM_RW_32_BIT,
+			((bus_num << BUS_NUM_SHIFT) | devfn));
+
+	return 0;
+}
+
+/* If device is secure dev, write config need transfer to tmm module */
+int virtcca_pci_generic_config_write(void __iomem *addr, unsigned char bus_num,
+	unsigned int devfn, int size, u32 val)
+{
+	if (size == 1)
+		WARN_ON(tmi_mmio_write(mmio_va_to_pa(addr), val,
+			CVM_RW_8_BIT, ((bus_num << BUS_NUM_SHIFT) | devfn)));
+	else if (size == 2)
+		WARN_ON(tmi_mmio_write(mmio_va_to_pa(addr), val,
+			CVM_RW_16_BIT, ((bus_num << BUS_NUM_SHIFT) | devfn)));
+	else
+		WARN_ON(tmi_mmio_write(mmio_va_to_pa(addr), val,
+			CVM_RW_32_BIT, ((bus_num << BUS_NUM_SHIFT) | devfn)));
+
+	return 0;
+}
+
+/* Judge startup virtcca_cvm_host is enable and device is secure or not */
+bool is_virtcca_pci_io_rw(struct vfio_pci_core_device *vdev)
+{
+	if (!is_virtcca_cvm_enable())
+		return false;
+
+	struct pci_dev *pdev = vdev->pdev;
+	bool cc_dev = pdev == NULL ? false : is_cc_dev(pci_dev_id(pdev));
+
+	if (cc_dev)
+		return true;
+
+	return false;
+}
+EXPORT_SYMBOL_GPL(is_virtcca_pci_io_rw);
+
+/* Transfer to tmm write io value */
+void virtcca_pci_io_write(struct vfio_pci_core_device *vdev, u64 val,
+	u64 size, void __iomem *io)
+{
+	struct pci_dev *pdev = vdev->pdev;
+
+	WARN_ON(tmi_mmio_write(mmio_va_to_pa(io), val, size, pci_dev_id(pdev)));
+}
+EXPORT_SYMBOL_GPL(virtcca_pci_io_write);
+
+/* Transfer to tmm read io value */
+u64 virtcca_pci_io_read(struct vfio_pci_core_device *vdev,
+	u64 size, void __iomem *io)
+{
+	struct pci_dev *pdev = vdev->pdev;
+
+	return tmi_mmio_read(mmio_va_to_pa(io), size, pci_dev_id(pdev));
+}
+EXPORT_SYMBOL_GPL(virtcca_pci_io_read);
+
+/* Whether the kvm is cvm */
+bool virtcca_iommu_domain_get_kvm(struct iommu_domain *domain, struct kvm **kvm)
+{
+	struct arm_smmu_domain *arm_smmu_domain;
+
+	arm_smmu_domain = to_smmu_domain(domain);
+	*kvm = virtcca_arm_smmu_get_kvm(arm_smmu_domain);
+	if (*kvm)
+		return (*kvm)->arch.is_virtcca_cvm;
+
+	return false;
+}
+EXPORT_SYMBOL_GPL(virtcca_iommu_domain_get_kvm);
+
+/**
+ * cvm_vfio_file_iommu_group - Get iommu group from vfio file
+ * @file: Vfio file
+ *
+ * Returns:
+ * %NULL if the virtcca_vfio_file_iommu_group func is not defined
+ * or CONFIG_HISI_VIRTCCA_HOST is not enable, group is null
+ * %iommu_group if get the iommu group from file success
+ */
+struct iommu_group *cvm_vfio_file_iommu_group(struct file *file)
+{
+	struct iommu_group *(*fn)(struct file *file);
+	struct iommu_group *ret;
+
+	fn = symbol_get(virtcca_vfio_file_iommu_group);
+	if (!fn)
+		return NULL;
+
+	ret = fn(file);
+
+	symbol_put(virtcca_vfio_file_iommu_group);
+
+	return ret;
+}
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index f3ab2c39f764..ca2abf7d98eb 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -19,7 +19,9 @@
 #include <asm/kvm_asm.h>
 #include <asm/kvm_emulate.h>
 #include <asm/virt.h>
-
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#include <asm/kvm_tmi.h>
+#endif
 #include "trace.h"
 
 static struct kvm_pgtable *hyp_pgtable;
@@ -1606,6 +1608,10 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 					     KVM_PGTABLE_WALK_HANDLE_FAULT |
 					     KVM_PGTABLE_WALK_SHARED);
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	ret = kvm_cvm_map_ipa(kvm, fault_ipa, pfn, vma_pagesize, prot, ret);
+#endif
+
 	/* Mark the page dirty only if the fault is handled successfully */
 	if (writable && !ret) {
 		kvm_set_pfn_dirty(pfn);
diff --git a/arch/arm64/kvm/tmi.c b/arch/arm64/kvm/tmi.c
index 769c00961222..cd38be6f2fba 100644
--- a/arch/arm64/kvm/tmi.c
+++ b/arch/arm64/kvm/tmi.c
@@ -6,6 +6,37 @@
 #include <asm/kvm_tmi.h>
 #include <asm/memory.h>
 
+/**
+ * mmio_va_to_pa - To convert the virtual address of the mmio space
+ * to a physical address, it is necessary to implement this interface
+ * because the kernel insterface __pa has an error when converting the
+ * physical address of the virtual address of the mmio space
+ * @addr:	MMIO virtual address
+ */
+u64 mmio_va_to_pa(void *addr)
+{
+	uint64_t pa, par_el1;
+
+	asm volatile(
+		"AT S1E1W, %0\n"
+		::"r"((uint64_t)(addr))
+	);
+	isb();
+	asm volatile(
+		"mrs %0, par_el1\n"
+		: "=r"(par_el1)
+	);
+
+	pa = ((uint64_t)(addr) & (PAGE_SIZE - 1)) |
+		(par_el1 & ULL(0x000ffffffffff000));
+
+	if (par_el1 & UL(1 << 0))
+		return (uint64_t)(addr);
+	else
+		return pa;
+}
+EXPORT_SYMBOL(mmio_va_to_pa);
+
 u64 tmi_version(void)
 {
 	struct arm_smccc_res res;
@@ -309,3 +340,12 @@ u64 tmi_smmu_read(u64 smmu_base, u64 reg_offset, u64 bits)
 }
 EXPORT_SYMBOL(tmi_smmu_read);
 
+/* Create device ttt */
+u64 tmi_dev_ttt_create(u64 numa_set, u64 rd, u64 map_addr, u64 level)
+{
+	struct arm_smccc_res res;
+
+	arm_smccc_1_1_smc(TMI_TMM_DEV_TTT_CREATE, numa_set, rd, map_addr, level, &res);
+	return res.a1;
+}
+
diff --git a/arch/arm64/kvm/virtcca_cvm.c b/arch/arm64/kvm/virtcca_cvm.c
index 93dd2392a1db..e299f9d17ff8 100644
--- a/arch/arm64/kvm/virtcca_cvm.c
+++ b/arch/arm64/kvm/virtcca_cvm.c
@@ -4,11 +4,15 @@
  */
 #include <linux/kvm_host.h>
 #include <linux/kvm.h>
+#include <linux/vfio.h>
+#include <linux/vfio_pci_core.h>
 #include <asm/kvm_tmi.h>
 #include <asm/kvm_pgtable.h>
 #include <asm/kvm_emulate.h>
 #include <asm/kvm_mmu.h>
 #include <asm/stage2_pgtable.h>
+#include <asm/virtcca_coda.h>
+#include <asm/virtcca_cvm_host.h>
 #include <linux/arm-smccc.h>
 #include <kvm/arm_hypercalls.h>
 #include <kvm/arm_psci.h>
@@ -175,10 +179,19 @@ void kvm_destroy_cvm(struct kvm *kvm)
 {
 	struct virtcca_cvm *cvm = kvm->arch.virtcca_cvm;
 	uint32_t cvm_vmid;
+	struct arm_smmu_domain *arm_smmu_domain;
+	struct list_head smmu_domain_group_list;
 
 	if (!cvm)
 		return;
 
+	/* Unmap the cvm with arm smmu domain */
+	kvm_get_arm_smmu_domain(kvm, &smmu_domain_group_list);
+	list_for_each_entry(arm_smmu_domain, &smmu_domain_group_list, node) {
+		if (arm_smmu_domain->kvm && arm_smmu_domain->kvm == kvm)
+			arm_smmu_domain->kvm = NULL;
+	}
+
 	cvm_vmid = cvm->cvm_vmid;
 	kfree(cvm->params);
 	cvm->params = NULL;
@@ -193,6 +206,7 @@ void kvm_destroy_cvm(struct kvm *kvm)
 	if (!tmi_cvm_destroy(cvm->rd))
 		kvm_info("KVM has destroyed cVM: %d\n", cvm->cvm_vmid);
 
+	cvm->is_mapped = false;
 	kfree(cvm);
 	kvm->arch.virtcca_cvm = NULL;
 }
@@ -514,20 +528,36 @@ int kvm_cvm_map_range(struct kvm *kvm)
 			}
 		}
 	}
-
+	/* Vfio driver will pin memory in advance,
+	 * if the ram already mapped, activate cvm
+	 * does not need to map twice
+	 */
+	cvm->is_mapped = true;
 	return ret;
 }
 
 static int kvm_activate_cvm(struct kvm *kvm)
 {
+	int ret;
+	struct arm_smmu_domain *arm_smmu_domain;
+	struct list_head smmu_domain_group_list;
 	struct virtcca_cvm *cvm = kvm->arch.virtcca_cvm;
 
 	if (virtcca_cvm_state(kvm) != CVM_STATE_NEW)
 		return -EINVAL;
 
-	if (kvm_cvm_map_range(kvm))
+	if (!cvm->is_mapped && kvm_cvm_map_range(kvm))
 		return -EFAULT;
 
+	kvm_get_arm_smmu_domain(kvm, &smmu_domain_group_list);
+	list_for_each_entry(arm_smmu_domain, &smmu_domain_group_list, node) {
+		if (arm_smmu_domain) {
+			ret = virtcca_smmu_tmi_dev_attach(arm_smmu_domain, kvm);
+			if (ret)
+				return ret;
+		}
+	}
+
 	if (tmi_cvm_activate(cvm->rd)) {
 		kvm_err("tmi_cvm_activate failed!\n");
 		return -ENXIO;
@@ -852,3 +882,281 @@ int kvm_init_cvm_vm(struct kvm *kvm)
 
 	return 0;
 }
+
+/*
+ * Coda (Confidential Device Assignment) feature
+ * enable devices to pass directly to confidential virtual machines
+ */
+
+/**
+ * is_in_virtcca_ram_range - Check if the iova belongs
+ * to the cvm ram range
+ * @kvm: The handle of kvm
+ * @iova: Ipa address
+ *
+ * Returns:
+ * %true if the iova belongs to cvm ram
+ * %false if the iova is not within the scope of cvm ram
+ */
+bool is_in_virtcca_ram_range(struct kvm *kvm, uint64_t iova)
+{
+	if (!is_virtcca_cvm_enable())
+		return false;
+
+	struct virtcca_cvm *virtcca_cvm = kvm->arch.virtcca_cvm;
+
+	if (iova >= virtcca_cvm->loader_start &&
+		iova < virtcca_cvm->loader_start + virtcca_cvm->ram_size)
+		return true;
+
+	return false;
+}
+EXPORT_SYMBOL_GPL(is_in_virtcca_ram_range);
+
+/**
+ * is_virtcca_iova_need_vfio_dma - Whether the vfio need
+ * to map the dma address
+ * @kvm: The handle of kvm
+ * @iova: Ipa address
+ *
+ * Returns:
+ * %true if virtcca cvm ram is nort mapped or
+ * virtcca_cvm_ram is mapped and the iova does not
+ * belong to cvm ram range
+ * %false if virtcca_cvm_ram is mapped and the iova belong
+ * to cvm ram range
+ */
+bool is_virtcca_iova_need_vfio_dma(struct kvm *kvm, uint64_t iova)
+{
+	if (!is_virtcca_cvm_enable())
+		return false;
+
+	struct virtcca_cvm *virtcca_cvm = kvm->arch.virtcca_cvm;
+
+	if (!virtcca_cvm->is_mapped)
+		return true;
+
+	return !is_in_virtcca_ram_range(kvm, iova);
+}
+EXPORT_SYMBOL_GPL(is_virtcca_iova_need_vfio_dma);
+
+/**
+ * cvm_arm_smmu_domain_set_kvm - Associate SMMU domain with CV
+ * @group: Iommu group
+ *
+ * Returns:
+ * %0 if smmu_domain has been associate cvm or associate cvm successfully
+ * %-ENXIO if the iommu group does not have smmu domain
+ */
+int cvm_arm_smmu_domain_set_kvm(void *group)
+{
+	struct arm_smmu_domain *arm_smmu_domain = NULL;
+	struct iommu_domain *domain;
+	struct kvm *kvm;
+
+	domain = virtcca_iommu_group_get_domain((struct iommu_group *)group);
+	if (!domain)
+		return -ENXIO;
+
+	arm_smmu_domain = to_smmu_domain(domain);
+	if (arm_smmu_domain->kvm)
+		return 0;
+
+	kvm = virtcca_arm_smmu_get_kvm(arm_smmu_domain);
+	if (kvm && kvm_is_virtcca_cvm(kvm))
+		arm_smmu_domain->kvm = kvm;
+
+	return 0;
+}
+
+static int kvm_cvm_dev_ttt_create(struct virtcca_cvm *cvm,
+			unsigned long addr,
+			int level,
+			u64 numa_set)
+{
+	addr = ALIGN_DOWN(addr, cvm_ttt_level_mapsize(level - 1));
+	return tmi_dev_ttt_create(numa_set, cvm->rd, addr, level);
+}
+
+/* CVM create ttt level information about device */
+int kvm_cvm_create_dev_ttt_levels(struct kvm *kvm, struct virtcca_cvm *cvm,
+			unsigned long ipa,
+			int level,
+			int max_level,
+			struct kvm_mmu_memory_cache *mc)
+{
+	int ret = 0;
+
+	while (level++ < max_level) {
+		u64 numa_set = kvm_get_first_binded_numa_set(kvm);
+
+		ret = kvm_cvm_dev_ttt_create(cvm, ipa, level, numa_set);
+		if (ret)
+			return -ENXIO;
+	}
+
+	return 0;
+}
+
+/**
+ * cvm_map_unmap_ipa_range - Vfio driver map or
+ * unmap cvm ipa
+ * @kvm: The handle of kvm
+ * @ipa_base: Ipa address
+ * @pa: Physical address
+ * @map_size: Map range
+ * @is_map: Map type
+ *
+ * Returns:
+ * %0 if cvm map/unmap address successfully
+ * %-ENXIO if map/unmap failed
+ */
+int cvm_map_unmap_ipa_range(struct kvm *kvm, phys_addr_t ipa_base,
+	phys_addr_t pa, unsigned long map_size, uint32_t is_map)
+{
+	unsigned long map_start;
+	unsigned long map_end;
+	int level = CVM_TTT_MAX_LEVEL;
+	struct virtcca_cvm *virtcca_cvm = kvm->arch.virtcca_cvm;
+	phys_addr_t rd = virtcca_cvm->rd;
+	unsigned long phys = pa;
+	int ret = 0;
+
+	map_start = ipa_base;
+	map_end = map_start + map_size;
+	while (map_start < map_end) {
+		if (is_map)
+			ret = tmi_mmio_map(rd, map_start, level, phys);
+		else
+			ret = tmi_mmio_unmap(rd, map_start, level);
+
+		if (TMI_RETURN_STATUS(ret) == TMI_ERROR_TTT_WALK) {
+			/* Create missing TTTs and retry */
+			int level_fault = TMI_RETURN_INDEX(ret);
+
+			if (is_map) {
+				ret = kvm_cvm_create_dev_ttt_levels(kvm, virtcca_cvm, map_start,
+					level_fault, CVM_TTT_MAX_LEVEL, NULL);
+				if (ret)
+					goto err;
+				ret = tmi_mmio_map(rd, map_start, level, phys);
+			} else {
+				ret = tmi_mmio_unmap(rd, map_start, level_fault);
+				map_size = tmm_granule_size(level_fault);
+			}
+		}
+
+		if (ret)
+			goto err;
+
+		map_start += map_size;
+		phys += map_size;
+	}
+
+	return 0;
+
+err:
+	if (!tmi_cvm_destroy(rd))
+		kvm_info("Vfio map failed, kvm has destroyed cVM: %d\n", virtcca_cvm->cvm_vmid);
+	return -ENXIO;
+}
+
+/**
+ * kvm_cvm_map_ipa_mmio - Map the mmio address when page fault
+ * @kvm: The handle of kvm
+ * @ipa_base: Ipa address
+ * @pa: Physical address
+ * @map_size: Map range
+ *
+ * Returns:
+ * %0 if cvm map address successfully
+ * %-ENXIO if map failed
+ */
+int kvm_cvm_map_ipa_mmio(struct kvm *kvm, phys_addr_t ipa_base,
+	phys_addr_t pa, unsigned long map_size)
+{
+	unsigned long size;
+	gfn_t gfn;
+	kvm_pfn_t pfn;
+	struct virtcca_cvm *virtcca_cvm = kvm->arch.virtcca_cvm;
+	phys_addr_t rd = virtcca_cvm->rd;
+	unsigned long ipa = ipa_base;
+	unsigned long phys = pa;
+	int ret = 0;
+
+	if (WARN_ON(!IS_ALIGNED(ipa, map_size)))
+		return -EINVAL;
+
+	for (size = 0; size < map_size; size += PAGE_SIZE) {
+		ret = tmi_mmio_map(rd, ipa, CVM_TTT_MAX_LEVEL, phys);
+		if (ret == TMI_ERROR_TTT_CREATED) {
+			ret = 0;
+			goto label;
+		}
+		if (TMI_RETURN_STATUS(ret) == TMI_ERROR_TTT_WALK) {
+			/* Create missing TTTs and retry */
+			int level_fault = TMI_RETURN_INDEX(ret);
+
+			ret = kvm_cvm_create_dev_ttt_levels(kvm, virtcca_cvm, ipa, level_fault,
+					CVM_TTT_MAX_LEVEL, NULL);
+
+			if (ret)
+				goto err;
+			ret = tmi_mmio_map(rd, ipa, CVM_TTT_MAX_LEVEL, phys);
+		}
+
+		if (ret)
+			goto err;
+label:
+		if (size + PAGE_SIZE >= map_size)
+			break;
+
+		ipa += PAGE_SIZE;
+		gfn = gpa_to_gfn(ipa);
+		pfn = gfn_to_pfn(kvm, gfn);
+		kvm_set_pfn_accessed(pfn);
+		kvm_release_pfn_clean(pfn);
+		phys = (uint64_t)__pfn_to_phys(pfn);
+
+	}
+
+	return 0;
+
+err:
+	if (!tmi_cvm_destroy(rd))
+		kvm_info("MMIO map failed, kvm has destroyed cVM: %d\n", virtcca_cvm->cvm_vmid);
+	return -ENXIO;
+}
+
+/* Page fault map ipa */
+int kvm_cvm_map_ipa(struct kvm *kvm, phys_addr_t ipa, kvm_pfn_t pfn,
+	unsigned long map_size, enum kvm_pgtable_prot prot, int ret)
+{
+	if (!is_virtcca_cvm_enable() || !kvm_is_virtcca_cvm(kvm))
+		return ret;
+
+	struct page *dst_page = pfn_to_page(pfn);
+	phys_addr_t dst_phys = page_to_phys(dst_page);
+
+	if (WARN_ON(!(prot & KVM_PGTABLE_PROT_W)))
+		return -EFAULT;
+
+	if (prot & KVM_PGTABLE_PROT_DEVICE)
+		return kvm_cvm_map_ipa_mmio(kvm, ipa, dst_phys, map_size);
+
+	return 0;
+}
+
+/* Set device secure flag */
+void virtcca_cvm_set_secure_flag(void *vdev, void *info)
+{
+	if (!is_virtcca_cvm_enable())
+		return;
+
+	if (!is_cc_dev(pci_dev_id(((struct vfio_pci_core_device *)vdev)->pdev)))
+		return;
+
+	((struct vfio_device_info *)info)->flags |= VFIO_DEVICE_FLAGS_SECURE;
+}
+EXPORT_SYMBOL_GPL(virtcca_cvm_set_secure_flag);
+
diff --git a/drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.c b/drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.c
index d247168c68f7..7612f673d2bf 100644
--- a/drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.c
+++ b/drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.c
@@ -13,6 +13,8 @@ struct cc_dev_config {
 	u32               vmid; /* virtual machine id */
 	u32               root_bd; /* root bus and device number. */
 	bool              secure; /* device secure attribute */
+	/* MSI addr for confidential device with iommu group granularity */
+	u64               msi_addr;
 	struct hlist_node node; /* device hash table */
 };
 
@@ -242,6 +244,42 @@ bool is_cc_dev(u32 sid)
 }
 EXPORT_SYMBOL(is_cc_dev);
 
+/**
+ * get_g_cc_dev_msi_addr - Obtain the msi address of confidential device
+ * @sid: Stream id of dev
+ *
+ * Returns:
+ * %0 if does not find the confidential device that matches the stream id
+ * %msi_addr return the msi address of confidential device that matches the stream id
+ */
+u64 get_g_cc_dev_msi_addr(u32 sid)
+{
+	struct cc_dev_config *obj;
+
+	hash_for_each_possible(g_cc_dev_htable, obj, node, sid) {
+		if (obj != NULL && obj->sid == sid)
+			return obj->msi_addr;
+	}
+	return 0;
+}
+
+/**
+ * set_g_cc_dev_msi_addr - Set the msi address of confidential device
+ * @sid: Stream id of dev
+ * @msi_addr: Msi address
+ */
+void set_g_cc_dev_msi_addr(u32 sid, u64 msi_addr)
+{
+	struct cc_dev_config *obj;
+
+	hash_for_each_possible(g_cc_dev_htable, obj, node, sid) {
+		if (obj != NULL && obj->sid == sid && !obj->msi_addr) {
+			obj->msi_addr = msi_addr;
+			return;
+		}
+	}
+}
+
 /**
  * virtcca_smmu_cmdq_need_forward - Whether the cmd queue need transfer to secure world
  * @cmd0: Command consists of 128 bits, cmd0 is the low 64 bits
@@ -650,7 +688,7 @@ u32 virtcca_smmu_tmi_dev_attach(struct arm_smmu_domain *arm_smmu_domain, struct
 	struct arm_smmu_master *master;
 	int ret = 0;
 	u64 cmd[CMDQ_ENT_DWORDS] = {0};
-	struct virtcca_cvm *virtcca_cvm = (struct virtcca_cvm *)kvm->arch.virtcca_cvm;
+	struct virtcca_cvm *virtcca_cvm = kvm->arch.virtcca_cvm;
 
 	spin_lock_irqsave(&arm_smmu_domain->devices_lock, flags);
 	/*
@@ -1172,6 +1210,7 @@ int virtcca_smmu_secure_dev_operator(struct iommu_domain *domain, struct device
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(virtcca_smmu_secure_dev_operator);
 
 /**
  * virtcca_smmu_device_init - Initialize the smmu security features
diff --git a/drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.h b/drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.h
index f5013354e841..cb155a1d59f8 100644
--- a/drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.h
+++ b/drivers/iommu/arm/arm-smmu-v3/arm-s-smmu-v3.h
@@ -171,6 +171,9 @@
 /* Has the root bus device number switched to secure */
 bool is_cc_dev(u32 sid);
 
+u64 get_g_cc_dev_msi_addr(u32 sid);
+void set_g_cc_dev_msi_addr(u32 sid, u64 msi_addr);
+
 void virtcca_smmu_cmdq_write_entries(struct arm_smmu_device *smmu, u64 *cmds,
 	struct arm_smmu_ll_queue *llq, struct arm_smmu_queue *q,
 	int n, bool sync);
@@ -178,7 +181,6 @@ bool virtcca_smmu_write_msi_msg(struct msi_desc *desc, struct msi_msg *msg);
 u32 virtcca_smmu_tmi_dev_attach(struct arm_smmu_domain *arm_smmu_domain,
 	struct kvm *kvm);
 void _arm_smmu_write_msi_msg(struct msi_desc *desc, struct msi_msg *msg);
-int virtcca_smmu_secure_dev_operator(struct iommu_domain *domain, struct device *dev);
 void virtcca_smmu_device_init(struct platform_device *pdev,
 	struct arm_smmu_device *smmu, resource_size_t ioaddr, bool resume, bool disable_bypass);
 
diff --git a/drivers/iommu/dma-iommu.c b/drivers/iommu/dma-iommu.c
index b733b1fb443e..1d6af11ef04a 100644
--- a/drivers/iommu/dma-iommu.c
+++ b/drivers/iommu/dma-iommu.c
@@ -33,6 +33,11 @@
 
 #include "dma-iommu.h"
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#include <asm/virtcca_coda.h>
+#include <asm/virtcca_cvm_host.h>
+#endif
+
 struct iommu_dma_msi_page {
 	struct list_head	list;
 	dma_addr_t		iova;
@@ -1811,6 +1816,47 @@ void iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 dma_limit)
 }
 EXPORT_SYMBOL_GPL(iommu_setup_dma_ops);
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+/* Virtcca map msi address */
+static struct iommu_dma_msi_page *virtcca_iommu_dma_get_msi_page(struct device *dev,
+	phys_addr_t msi_addr, struct iommu_domain *domain)
+{
+	struct iommu_dma_cookie *cookie = domain->iova_cookie;
+	struct iommu_dma_msi_page *msi_page;
+	dma_addr_t iova;
+	int prot = IOMMU_WRITE | IOMMU_NOEXEC | IOMMU_MMIO;
+	size_t size = cookie_msi_granule(cookie);
+
+	msi_addr &= ~(phys_addr_t)(size - 1);
+	list_for_each_entry(msi_page, &cookie->msi_page_list, list)
+		if (msi_page->phys == msi_addr)
+			return msi_page;
+
+	msi_page = kzalloc(sizeof(*msi_page), GFP_KERNEL);
+	if (!msi_page)
+		return NULL;
+
+	iova = iommu_dma_alloc_iova(domain, size, dma_get_mask(dev), dev);
+	if (!iova)
+		goto out_free_page;
+
+	if (virtcca_iommu_map(domain, iova, msi_addr, size, prot))
+		goto out_free_iova;
+
+	INIT_LIST_HEAD(&msi_page->list);
+	msi_page->phys = msi_addr;
+	msi_page->iova = iova;
+	list_add(&msi_page->list, &cookie->msi_page_list);
+	return msi_page;
+
+out_free_iova:
+	iommu_dma_free_iova(cookie, iova, size, NULL);
+out_free_page:
+	kfree(msi_page);
+	return NULL;
+}
+#endif
+
 static struct iommu_dma_msi_page *iommu_dma_get_msi_page(struct device *dev,
 		phys_addr_t msi_addr, struct iommu_domain *domain)
 {
@@ -1820,6 +1866,10 @@ static struct iommu_dma_msi_page *iommu_dma_get_msi_page(struct device *dev,
 	int prot = IOMMU_WRITE | IOMMU_NOEXEC | IOMMU_MMIO;
 	size_t size = cookie_msi_granule(cookie);
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (is_virtcca_cvm_enable() && domain->secure)
+		return virtcca_iommu_dma_get_msi_page(dev, msi_addr, domain);
+#endif
 	msi_addr &= ~(phys_addr_t)(size - 1);
 	list_for_each_entry(msi_page, &cookie->msi_page_list, list)
 		if (msi_page->phys == msi_addr)
diff --git a/drivers/iommu/io-pgtable-arm.c b/drivers/iommu/io-pgtable-arm.c
index fb54baed3f49..e8a716c5d0a1 100644
--- a/drivers/iommu/io-pgtable-arm.c
+++ b/drivers/iommu/io-pgtable-arm.c
@@ -20,6 +20,10 @@
 
 #include <asm/barrier.h>
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#include <asm/virtcca_coda.h>
+#endif
+
 #include "io-pgtable-arm.h"
 
 #define ARM_LPAE_MAX_ADDR_BITS		52
@@ -1723,3 +1727,23 @@ static int __init arm_lpae_do_selftests(void)
 }
 subsys_initcall(arm_lpae_do_selftests);
 #endif
+
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+/* Obtain io pgtable data from io pgtable ops */
+struct arm_lpae_io_pgtable *virtcca_io_pgtable_get_data(void *ops)
+{
+	return io_pgtable_ops_to_data((struct io_pgtable_ops *)ops);
+}
+
+/* Obtain io pgtable cfg from io pgtable data */
+struct io_pgtable_cfg *virtcca_io_pgtable_get_cfg(struct arm_lpae_io_pgtable *data)
+{
+	return &data->iop.cfg;
+}
+
+/* Obtain smmu domain from io pgtable data */
+void *virtcca_io_pgtable_get_smmu_domain(struct arm_lpae_io_pgtable *data)
+{
+	return data->iop.cookie;
+}
+#endif
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index 28f63ad432de..3c29b4b8fcca 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -33,6 +33,12 @@
 #include <linux/sched/mm.h>
 #include <linux/msi.h>
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#ifndef __GENKSYMS__
+#include <asm/virtcca_coda.h>
+#endif
+#endif
+
 #include "dma-iommu.h"
 #include "iommu-priv.h"
 
@@ -3654,3 +3660,72 @@ void iommu_free_global_pasid(ioasid_t pasid)
 	ida_free(&iommu_global_pasid_ida, pasid);
 }
 EXPORT_SYMBOL_GPL(iommu_free_global_pasid);
+
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+/**
+ * virtcca_attach_secure_dev - Attach the device of iommu
+ * group to confidential virtual machine
+ * @domain: The handle of iommu domain
+ * @group: Iommu group
+ *
+ * Returns:
+ * %0 if attach the all devices success
+ * %-EINVAL if the smmu does not initialize secure state
+ * %-ENOMEM if the device create secure ste failed
+ * %-ENOENT if the device does not have fwspec
+ */
+int virtcca_attach_secure_dev(struct iommu_domain *domain, struct iommu_group *group)
+{
+	struct group_device *gdev;
+	int ret = 0;
+
+	mutex_lock(&group->mutex);
+	for_each_group_device(group, gdev)
+		ret = virtcca_smmu_secure_dev_operator(domain, gdev->dev);
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(virtcca_attach_secure_dev);
+
+/* Obtain domain information through iommu group */
+struct iommu_domain *virtcca_iommu_group_get_domain(struct iommu_group *iommu_group)
+{
+	if (iommu_group)
+		return iommu_group->domain;
+
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(virtcca_iommu_group_get_domain);
+
+/* Obtain msi address through iommu group id */
+u64 virtcca_get_iommu_device_msi_addr(struct iommu_group *iommu_group)
+{
+	u64 msi_addr = iommu_group->id * CVM_MSI_IOVA_OFFSET + CVM_MSI_MIN_IOVA;
+
+	if (msi_addr >= CVM_MSI_MAX_IOVA) {
+		pr_err("MSI address overflow.\n");
+		return 0;
+	}
+
+	return msi_addr;
+}
+EXPORT_SYMBOL_GPL(virtcca_get_iommu_device_msi_addr);
+
+/* Traverse the devices in the iommu group and set the corresponding MSI address */
+void virtcca_set_dev_msi_addr(struct iommu_group *iommu_group, unsigned long iova)
+{
+	struct group_device *group_device;
+	struct device *dev;
+
+	mutex_lock(&iommu_group->mutex);
+	list_for_each_entry(group_device, &iommu_group->devices, list) {
+		dev = group_device->dev;
+		struct pci_dev *pci_dev = to_pci_dev(dev);
+		u16 pci_id = pci_dev_id(pci_dev);
+
+		set_g_cc_dev_msi_addr(pci_id, iova);
+	}
+	mutex_unlock(&iommu_group->mutex);
+}
+EXPORT_SYMBOL_GPL(virtcca_set_dev_msi_addr);
+#endif
diff --git a/drivers/pci/access.c b/drivers/pci/access.c
index 6554a2e89d36..2f88c9dcc2ce 100644
--- a/drivers/pci/access.c
+++ b/drivers/pci/access.c
@@ -5,6 +5,15 @@
 #include <linux/ioport.h>
 #include <linux/wait.h>
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#ifndef __GENKSYMS__
+#include <linux/kvm_host.h>
+#include <asm/kvm_tmm.h>
+#include <asm/virtcca_coda.h>
+#include <asm/virtcca_cvm_host.h>
+#endif
+#endif
+
 #include "pci.h"
 
 /*
@@ -86,6 +95,11 @@ int pci_generic_config_read(struct pci_bus *bus, unsigned int devfn,
 	if (!addr)
 		return PCIBIOS_DEVICE_NOT_FOUND;
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (is_virtcca_cvm_enable() && is_cc_dev((bus->number << BUS_NUM_SHIFT) | devfn))
+		return virtcca_pci_generic_config_read(addr, bus->number, devfn, size, val);
+#endif
+
 	if (size == 1)
 		*val = readb(addr);
 	else if (size == 2)
@@ -106,6 +120,11 @@ int pci_generic_config_write(struct pci_bus *bus, unsigned int devfn,
 	if (!addr)
 		return PCIBIOS_DEVICE_NOT_FOUND;
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (is_virtcca_cvm_enable() && is_cc_dev((bus->number << BUS_NUM_SHIFT) | devfn))
+		return virtcca_pci_generic_config_write(addr, bus->number, devfn, size, val);
+#endif
+
 	if (size == 1)
 		writeb(val, addr);
 	else if (size == 2)
diff --git a/drivers/pci/msi/msi.c b/drivers/pci/msi/msi.c
index 161c3ac171a3..d2c9863d0368 100644
--- a/drivers/pci/msi/msi.c
+++ b/drivers/pci/msi/msi.c
@@ -13,6 +13,10 @@
 #include "../pci.h"
 #include "msi.h"
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#include <asm/virtcca_cvm_host.h>
+#endif
+
 int pci_msi_enable = 1;
 int pci_msi_ignore_mask;
 
@@ -159,6 +163,10 @@ void __pci_read_msi_msg(struct msi_desc *entry, struct msi_msg *msg)
 		if (WARN_ON_ONCE(entry->pci.msi_attrib.is_virtual))
 			return;
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+		if (is_virtcca_cvm_enable() && dev != NULL && is_cc_dev(pci_dev_id(dev)))
+			return virtcca_pci_read_msi_msg(dev, msg, base);
+#endif
 		msg->address_lo = readl(base + PCI_MSIX_ENTRY_LOWER_ADDR);
 		msg->address_hi = readl(base + PCI_MSIX_ENTRY_UPPER_ADDR);
 		msg->data = readl(base + PCI_MSIX_ENTRY_DATA);
@@ -221,6 +229,10 @@ static inline void pci_write_msg_msix(struct msi_desc *desc, struct msi_msg *msg
 	if (unmasked)
 		pci_msix_write_vector_ctrl(desc, ctrl | PCI_MSIX_ENTRY_CTRL_MASKBIT);
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (virtcca_pci_write_msg_msi(desc, msg))
+		return;
+#endif
 	writel(msg->address_lo, base + PCI_MSIX_ENTRY_LOWER_ADDR);
 	writel(msg->address_hi, base + PCI_MSIX_ENTRY_UPPER_ADDR);
 	writel(msg->data, base + PCI_MSIX_ENTRY_DATA);
@@ -639,6 +651,10 @@ void msix_prepare_msi_desc(struct pci_dev *dev, struct msi_desc *desc)
 	if (desc->pci.msi_attrib.can_mask) {
 		void __iomem *addr = pci_msix_desc_addr(desc);
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+		if (is_virtcca_cvm_enable() && is_cc_dev(pci_dev_id(dev)))
+			return virtcca_msix_prepare_msi_desc(dev, desc, addr);
+#endif
 		desc->pci.msix_ctrl = readl(addr + PCI_MSIX_ENTRY_VECTOR_CTRL);
 	}
 }
@@ -776,6 +792,10 @@ static int msix_capability_init(struct pci_dev *dev, struct msix_entry *entries,
 	 * which takes the MSI-X mask bits into account even
 	 * when MSI-X is disabled, which prevents MSI delivery.
 	 */
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (is_virtcca_cvm_enable() && is_cc_dev(pci_dev_id(dev)))
+		return msix_mask_all_cc(dev, dev->msix_base, tsize, pci_dev_id(dev));
+#endif
 	msix_mask_all(dev->msix_base, tsize);
 	pci_msix_clear_and_set_ctrl(dev, PCI_MSIX_FLAGS_MASKALL, 0);
 
diff --git a/drivers/pci/msi/msi.h b/drivers/pci/msi/msi.h
index ee53cf079f4e..154e2c00f94c 100644
--- a/drivers/pci/msi/msi.h
+++ b/drivers/pci/msi/msi.h
@@ -3,6 +3,12 @@
 #include <linux/pci.h>
 #include <linux/msi.h>
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#ifndef __GENKSYMS__
+#include <asm/virtcca_coda.h>
+#endif
+#endif
+
 #define msix_table_size(flags)	((flags & PCI_MSIX_FLAGS_QSIZE) + 1)
 
 int pci_msi_setup_msi_irqs(struct pci_dev *dev, int nvec, int type);
@@ -36,6 +42,11 @@ static inline void pci_msix_write_vector_ctrl(struct msi_desc *desc, u32 ctrl)
 {
 	void __iomem *desc_addr = pci_msix_desc_addr(desc);
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (virtcca_pci_msix_write_vector_ctrl(desc, ctrl))
+		return;
+#endif
+
 	if (desc->pci.msi_attrib.can_mask)
 		writel(ctrl, desc_addr + PCI_MSIX_ENTRY_VECTOR_CTRL);
 }
@@ -44,6 +55,11 @@ static inline void pci_msix_mask(struct msi_desc *desc)
 {
 	desc->pci.msix_ctrl |= PCI_MSIX_ENTRY_CTRL_MASKBIT;
 	pci_msix_write_vector_ctrl(desc, desc->pci.msix_ctrl);
+
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (virtcca_pci_msix_mask(desc))
+		return;
+#endif
 	/* Flush write to device */
 	readl(desc->pci.mask_base);
 }
diff --git a/drivers/vfio/group.c b/drivers/vfio/group.c
index 610a429c6191..33a66b6c5759 100644
--- a/drivers/vfio/group.c
+++ b/drivers/vfio/group.c
@@ -957,3 +957,34 @@ void vfio_group_cleanup(void)
 	vfio.class = NULL;
 	vfio_container_cleanup();
 }
+
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+/**
+ * virtcca_vfio_file_iommu_group - Return the struct iommu_group for the vfio group file
+ * @file: VFIO group file
+ *
+ * The returned iommu_group is valid as long as a ref is held on the file. This
+ * returns a reference on the group. This function is deprecated, only the SPAPR
+ * path in kvm should call it.
+ */
+struct iommu_group *virtcca_vfio_file_iommu_group(struct file *file)
+{
+	struct vfio_group *group = vfio_group_from_file(file);
+	struct iommu_group *iommu_group = NULL;
+
+	if (!IS_ENABLED(CONFIG_HISI_VIRTCCA_HOST))
+		return NULL;
+
+	if (!group)
+		return NULL;
+
+	mutex_lock(&group->group_lock);
+	if (group->iommu_group) {
+		iommu_group = group->iommu_group;
+		iommu_group_ref_get(iommu_group);
+	}
+	mutex_unlock(&group->group_lock);
+	return iommu_group;
+}
+EXPORT_SYMBOL_GPL(virtcca_vfio_file_iommu_group);
+#endif
diff --git a/drivers/vfio/pci/vfio_pci_core.c b/drivers/vfio/pci/vfio_pci_core.c
index 0524c90cedea..2bff38b56f97 100644
--- a/drivers/vfio/pci/vfio_pci_core.c
+++ b/drivers/vfio/pci/vfio_pci_core.c
@@ -31,6 +31,9 @@
 #if IS_ENABLED(CONFIG_EEH)
 #include <asm/eeh.h>
 #endif
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#include <asm/kvm_tmi.h>
+#endif
 
 #include "vfio_pci_priv.h"
 
@@ -973,6 +976,10 @@ static int vfio_pci_ioctl_get_info(struct vfio_pci_core_device *vdev,
 	if (vdev->reset_works)
 		info.flags |= VFIO_DEVICE_FLAGS_RESET;
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	virtcca_cvm_set_secure_flag((void *)vdev, (void *)&info);
+#endif
+
 	info.num_regions = VFIO_PCI_NUM_REGIONS + vdev->num_regions;
 	info.num_irqs = VFIO_PCI_NUM_IRQS;
 
diff --git a/drivers/vfio/pci/vfio_pci_rdwr.c b/drivers/vfio/pci/vfio_pci_rdwr.c
index e27de61ac9fe..d7eda4ca1de9 100644
--- a/drivers/vfio/pci/vfio_pci_rdwr.c
+++ b/drivers/vfio/pci/vfio_pci_rdwr.c
@@ -19,6 +19,13 @@
 
 #include "vfio_pci_priv.h"
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#ifndef __GENKSYMS__
+#include <asm/virtcca_coda.h>
+#include <asm/virtcca_cvm_host.h>
+#endif
+#endif
+
 #ifdef __LITTLE_ENDIAN
 #define vfio_ioread64	ioread64
 #define vfio_iowrite64	iowrite64
@@ -37,6 +44,31 @@
 #define vfio_ioread8	ioread8
 #define vfio_iowrite8	iowrite8
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#define VFIO_IOWRITE(size) \
+static int vfio_pci_iowrite##size(struct vfio_pci_core_device *vdev,	\
+			bool test_mem, u##size val, void __iomem *io)	\
+{									\
+	if (test_mem) {							\
+		down_read(&vdev->memory_lock);				\
+		if (!__vfio_pci_memory_enabled(vdev)) {			\
+			up_read(&vdev->memory_lock);			\
+			return -EIO;					\
+		}							\
+	}								\
+									\
+	if (is_virtcca_pci_io_rw(vdev)) {				\
+		virtcca_pci_io_write(vdev, val, size, io);		\
+	} else {							\
+		vfio_iowrite##size(val, io);				\
+	}								\
+									\
+	if (test_mem)							\
+		up_read(&vdev->memory_lock);				\
+									\
+	return 0;							\
+}
+#else
 #define VFIO_IOWRITE(size) \
 static int vfio_pci_iowrite##size(struct vfio_pci_core_device *vdev,		\
 			bool test_mem, u##size val, void __iomem *io)	\
@@ -56,6 +88,7 @@ static int vfio_pci_iowrite##size(struct vfio_pci_core_device *vdev,		\
 									\
 	return 0;							\
 }
+#endif
 
 VFIO_IOWRITE(8)
 VFIO_IOWRITE(16)
@@ -64,6 +97,31 @@ VFIO_IOWRITE(32)
 VFIO_IOWRITE(64)
 #endif
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#define VFIO_IOREAD(size) \
+static int vfio_pci_ioread##size(struct vfio_pci_core_device *vdev,	\
+			bool test_mem, u##size * val, void __iomem *io)	\
+{									\
+	if (test_mem) {							\
+		down_read(&vdev->memory_lock);				\
+		if (!__vfio_pci_memory_enabled(vdev)) {			\
+			up_read(&vdev->memory_lock);			\
+			return -EIO;					\
+		}							\
+	}								\
+									\
+	if (is_virtcca_pci_io_rw(vdev)) {				\
+		*val = virtcca_pci_io_read(vdev, size, io);		\
+	} else {							\
+		*val = vfio_ioread##size(io);				\
+	}								\
+									\
+	if (test_mem)							\
+		up_read(&vdev->memory_lock);				\
+									\
+	return 0;							\
+}
+#else
 #define VFIO_IOREAD(size) \
 static int vfio_pci_ioread##size(struct vfio_pci_core_device *vdev,		\
 			bool test_mem, u##size *val, void __iomem *io)	\
@@ -83,6 +141,7 @@ static int vfio_pci_ioread##size(struct vfio_pci_core_device *vdev,		\
 									\
 	return 0;							\
 }
+#endif
 
 VFIO_IOREAD(8)
 VFIO_IOREAD(16)
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index 9c4adf11dbbe..b33c145520ad 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -38,6 +38,12 @@
 #include <linux/workqueue.h>
 #include <linux/notifier.h>
 #include "vfio.h"
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#include <linux/kvm_host.h>
+#include <asm/kvm_tmm.h>
+#include <asm/virtcca_coda.h>
+#include <asm/virtcca_cvm_host.h>
+#endif
 
 #define DRIVER_VERSION  "0.2"
 #define DRIVER_AUTHOR   "Alex Williamson <alex.williamson@redhat.com>"
@@ -77,6 +83,9 @@ struct vfio_iommu {
 	bool			dirty_page_tracking;
 	struct list_head	emulated_iommu_groups;
 	bool			dirty_log_get_no_clear;
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	bool            secure;
+#endif
 };
 
 struct vfio_domain {
@@ -1037,6 +1046,220 @@ static size_t unmap_unpin_slow(struct vfio_domain *domain,
 	return unmapped;
 }
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+static void vfio_remove_dma(struct vfio_iommu *iommu, struct vfio_dma *dma);
+
+bool virtcca_check_kvm_is_cvm(void *iommu, struct kvm **kvm)
+{
+	struct vfio_domain *domain;
+	bool is_virtcca_cvm = false;
+	struct vfio_iommu *vfio_iommu = (struct vfio_iommu *)iommu;
+
+	if (!vfio_iommu || !kvm)
+		return false;
+
+	list_for_each_entry(domain, &vfio_iommu->domain_list, next) {
+		if (domain && domain->domain && virtcca_iommu_domain_get_kvm(domain->domain, kvm)) {
+			is_virtcca_cvm = true;
+			break;
+		}
+	}
+
+	return is_virtcca_cvm;
+}
+
+/* Traverse all domains and perform mapping operations */
+int virtcca_vfio_iommu_map(void *iommu, dma_addr_t iova,
+	unsigned long pfn, long npage, int prot)
+{
+	struct vfio_domain *d;
+	int ret;
+	struct vfio_iommu *vfio_iommu = (struct vfio_iommu *)iommu;
+
+	list_for_each_entry(d, &vfio_iommu->domain_list, next) {
+		ret = virtcca_iommu_map(d->domain, iova, (phys_addr_t)pfn << PAGE_SHIFT,
+			npage << PAGE_SHIFT, prot | IOMMU_CACHE);
+		if (ret)
+			goto unwind;
+
+		cond_resched();
+	}
+
+	return 0;
+
+unwind:
+	list_for_each_entry_continue_reverse(d, &vfio_iommu->domain_list, next) {
+		virtcca_iommu_unmap(d->domain, iova, npage << PAGE_SHIFT);
+		cond_resched();
+	}
+
+	return ret;
+}
+
+/**
+ * virtcca_vfio_pin_map_dma - Vfio need to map iova
+ * @iommu: The handle of iommu
+ * @dma: Dma information
+ * @map_size: Map size
+ *
+ * Returns:
+ * %0 if map success
+ */
+static int virtcca_vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
+	size_t map_size)
+{
+	struct kvm *kvm;
+	long npage;
+	dma_addr_t iova = dma->iova;
+	unsigned long vaddr = dma->vaddr;
+	struct vfio_batch batch;
+	size_t size = map_size;
+	unsigned long pfn, limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+	int ret = 0;
+	bool is_virtcca_cvm = virtcca_check_kvm_is_cvm((void *)iommu, &kvm);
+
+	vfio_batch_init(&batch);
+
+	while (size) {
+
+		if (is_virtcca_cvm && !is_virtcca_iova_need_vfio_dma(kvm, dma->iova))
+			break;
+		/* Pin a contiguous chunk of memory */
+		npage = vfio_pin_pages_remote(dma, vaddr + dma->size,
+					      size >> PAGE_SHIFT, &pfn, limit,
+					      &batch);
+		if (npage <= 0) {
+			WARN_ON(!npage);
+			ret = (int)npage;
+			break;
+		}
+
+		/* Map it! */
+		ret = virtcca_vfio_iommu_map(iommu, iova + dma->size, pfn, npage,
+				     dma->prot);
+		if (ret) {
+			vfio_unpin_pages_remote(dma, iova + dma->size, pfn,
+						npage, true);
+			vfio_batch_unpin(&batch, dma);
+			break;
+		}
+
+		if (is_virtcca_cvm && is_in_virtcca_ram_range(kvm, iova)) {
+			vfio_unpin_pages_remote(dma, iova + dma->size, pfn,
+				npage, true);
+			vfio_batch_unpin(&batch, dma);
+		}
+		size -= npage << PAGE_SHIFT;
+		dma->size += npage << PAGE_SHIFT;
+	}
+
+	vfio_batch_fini(&batch);
+	dma->iommu_mapped = true;
+
+	if (ret)
+		vfio_remove_dma(iommu, dma);
+
+	return ret;
+}
+
+/**
+ * virtcca_vfio_unmap_unpin - Vfio need to unmap iova
+ * @iommu: The handle of iommu
+ * @dma: Dma information
+ * @do_accounting: Need to account or not
+ *
+ * Returns:
+ * %0 if unmap success or the cvm is destroyed
+ */
+static long virtcca_vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
+	bool do_accounting)
+{
+	struct kvm *kvm;
+	struct vfio_domain *domain, *d;
+	LIST_HEAD(unmapped_region_list);
+	struct iommu_iotlb_gather iotlb_gather;
+	int unmapped_region_cnt = 0;
+	long unlocked = 0;
+	dma_addr_t iova = dma->iova, end = dma->iova + dma->size;
+
+	if (!virtcca_check_kvm_is_cvm((void *)iommu, &kvm))
+		return 0;
+
+	if (!dma->size)
+		return 0;
+
+	if (list_empty(&iommu->domain_list))
+		return 0;
+
+	/*
+	 * We use the IOMMU to track the physical addresses, otherwise we'd
+	 * need a much more complicated tracking system.  Unfortunately that
+	 * means we need to use one of the iommu domains to figure out the
+	 * pfns to unpin.  The rest need to be unmapped in advance so we have
+	 * no iommu translations remaining when the pages are unpinned.
+	 */
+	domain = d = list_first_entry(&iommu->domain_list,
+			struct vfio_domain, next);
+
+	list_for_each_entry_continue(d, &iommu->domain_list, next) {
+		virtcca_iommu_unmap(d->domain, dma->iova, dma->size);
+		cond_resched();
+	}
+
+	iommu_iotlb_gather_init(&iotlb_gather);
+	while (iova < end) {
+		size_t unmapped, len;
+		phys_addr_t phys, next;
+
+		phys = iommu_iova_to_phys(domain->domain, iova);
+		if (WARN_ON(!phys)) {
+			iova += PAGE_SIZE;
+			continue;
+		}
+
+		/*
+		 * To optimize for fewer iommu_unmap() calls, each of which
+		 * may require hardware cache flushing, try to find the
+		 * largest contiguous physical memory chunk to unmap.
+		 */
+		for (len = PAGE_SIZE;
+			!domain->fgsp && iova + len < end; len += PAGE_SIZE) {
+			next = iommu_iova_to_phys(domain->domain, iova + len);
+			if (next != phys + len)
+				break;
+		}
+
+		/*
+		 * First, try to use fast unmap/unpin. In case of failure,
+		 * switch to slow unmap/unpin path.
+		 */
+		unmapped = unmap_unpin_fast(domain, dma, &iova, len, phys,
+				&unlocked, &unmapped_region_list,
+				&unmapped_region_cnt,
+				&iotlb_gather);
+		if (!unmapped) {
+			unmapped = unmap_unpin_slow(domain, dma, &iova, len,
+					phys, &unlocked);
+			if (WARN_ON(!unmapped))
+				break;
+		}
+	}
+
+	dma->iommu_mapped = false;
+
+	if (unmapped_region_cnt) {
+		unlocked += vfio_sync_unpin(dma, domain, &unmapped_region_list,
+			&iotlb_gather);
+	}
+
+	if (do_accounting) {
+		vfio_lock_acct(dma, -unlocked, true);
+		return 0;
+	}
+	return unlocked;
+}
+#endif
+
 static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
 			     bool do_accounting)
 {
@@ -1047,6 +1270,11 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
 	int unmapped_region_cnt = 0;
 	long unlocked = 0;
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (is_virtcca_cvm_enable() && iommu->secure)
+		return virtcca_vfio_unmap_unpin(iommu, dma, do_accounting);
+#endif
+
 	if (!dma->size)
 		return 0;
 
@@ -1630,6 +1858,11 @@ static int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
 	unsigned long pfn, limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 	int ret = 0;
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (is_virtcca_cvm_enable() && iommu->secure)
+		return virtcca_vfio_pin_map_dma(iommu, dma, map_size);
+#endif
+
 	vfio_batch_init(&batch);
 
 	while (size) {
@@ -1853,6 +2086,11 @@ static int vfio_iommu_replay(struct vfio_iommu *iommu,
 	unsigned long limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 	int ret;
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (is_virtcca_cvm_enable() && iommu->secure)
+		return 0;
+#endif
+
 	/* Arbitrarily pick the first domain in the list for lookups */
 	if (!list_empty(&iommu->domain_list))
 		d = list_first_entry(&iommu->domain_list,
@@ -2454,10 +2692,23 @@ static int vfio_iommu_type1_attach_group(void *iommu_data,
 			goto out_domain;
 	}
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (iommu->secure)
+		domain->domain->secure = true;
+#endif
+
 	ret = iommu_attach_group(domain->domain, group->iommu_group);
 	if (ret)
 		goto out_domain;
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	if (iommu->secure) {
+		ret = virtcca_attach_secure_dev(domain->domain, group->iommu_group);
+		if (ret)
+			goto out_domain;
+	}
+#endif
+
 	/* Get aperture info */
 	geo = &domain->domain->geometry;
 	if (vfio_iommu_aper_conflict(iommu, geo->aperture_start,
@@ -2807,6 +3058,12 @@ static void *vfio_iommu_type1_open(unsigned long arg)
 	case VFIO_TYPE1v2_IOMMU:
 		iommu->v2 = true;
 		break;
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	case VFIO_TYPE1v2_S_IOMMU:
+		iommu->v2 = true;
+		iommu->secure = true;
+		break;
+#endif
 	default:
 		kfree(iommu);
 		return ERR_PTR(-EINVAL);
@@ -2898,6 +3155,9 @@ static int vfio_iommu_type1_check_extension(struct vfio_iommu *iommu,
 	switch (arg) {
 	case VFIO_TYPE1_IOMMU:
 	case VFIO_TYPE1v2_IOMMU:
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	case VFIO_TYPE1v2_S_IOMMU:
+#endif
 	case VFIO_TYPE1_NESTING_IOMMU:
 	case VFIO_UNMAP_ALL:
 		return 1;
diff --git a/include/linux/pci.h b/include/linux/pci.h
index 82f3571a36d9..54e340c81972 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -2710,6 +2710,10 @@ static inline bool pci_is_thunderbolt_attached(struct pci_dev *pdev)
 void pci_uevent_ers(struct pci_dev *pdev, enum  pci_ers_result err_type);
 #endif
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+bool is_cc_dev(u32 sid);
+#endif
+
 #include <linux/dma-mapping.h>
 
 #define pci_printk(level, pdev, fmt, arg...) \
diff --git a/include/linux/vfio.h b/include/linux/vfio.h
index 5ac5f182ce0b..28c14e7cf6ce 100644
--- a/include/linux/vfio.h
+++ b/include/linux/vfio.h
@@ -291,6 +291,10 @@ void vfio_combine_iova_ranges(struct rb_root_cached *root, u32 cur_nodes,
  */
 struct iommu_group *vfio_file_iommu_group(struct file *file);
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+struct iommu_group *virtcca_vfio_file_iommu_group(struct file *file);
+#endif
+
 #if IS_ENABLED(CONFIG_VFIO_GROUP)
 bool vfio_file_is_group(struct file *file);
 bool vfio_file_has_dev(struct file *file, struct vfio_device *device);
@@ -358,5 +362,4 @@ int vfio_virqfd_enable(void *opaque, int (*handler)(void *, void *),
 		       struct virqfd **pvirqfd, int fd);
 void vfio_virqfd_disable(struct virqfd **pvirqfd);
 void vfio_virqfd_flush_thread(struct virqfd **pvirqfd);
-
 #endif /* VFIO_H */
diff --git a/include/uapi/linux/vfio.h b/include/uapi/linux/vfio.h
index 2b78d03c0c15..df6cf5661d40 100644
--- a/include/uapi/linux/vfio.h
+++ b/include/uapi/linux/vfio.h
@@ -25,6 +25,7 @@
 #define VFIO_TYPE1_IOMMU		1
 #define VFIO_SPAPR_TCE_IOMMU		2
 #define VFIO_TYPE1v2_IOMMU		3
+#define VFIO_TYPE1v2_S_IOMMU		12
 /*
  * IOMMU enforces DMA cache coherence (ex. PCIe NoSnoop stripping).  This
  * capability is subject to change as groups are added or removed.
@@ -224,6 +225,7 @@ struct vfio_device_info {
 #define VFIO_DEVICE_FLAGS_FSL_MC (1 << 6)	/* vfio-fsl-mc device */
 #define VFIO_DEVICE_FLAGS_CAPS	(1 << 7)	/* Info supports caps */
 #define VFIO_DEVICE_FLAGS_CDX	(1 << 8)	/* vfio-cdx device */
+#define VFIO_DEVICE_FLAGS_SECURE (1 << 9)	/* vfio-secure device */
 	__u32	num_regions;	/* Max region index + 1 */
 	__u32	num_irqs;	/* Max IRQ index + 1 */
 	__u32   cap_offset;	/* Offset within info struct of first cap */
diff --git a/virt/kvm/vfio.c b/virt/kvm/vfio.c
index ca24ce120906..a890f9c05d44 100644
--- a/virt/kvm/vfio.c
+++ b/virt/kvm/vfio.c
@@ -21,6 +21,11 @@
 #include <asm/kvm_ppc.h>
 #endif
 
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+#include <asm/virtcca_coda.h>
+#include <asm/virtcca_cvm_host.h>
+#endif
+
 struct kvm_vfio_file {
 	struct list_head node;
 	struct file *file;
@@ -178,6 +183,9 @@ static int kvm_vfio_file_add(struct kvm_device *dev, unsigned int fd)
 	kvm_arch_start_assignment(dev->kvm);
 	kvm_vfio_file_set_kvm(kvf->file, dev->kvm);
 	kvm_vfio_update_coherency(dev);
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+	ret = cvm_vfio_add_kvm_to_smmu_domain(filp, (void *)kv);
+#endif
 
 out_unlock:
 	mutex_unlock(&kv->lock);
@@ -392,3 +400,221 @@ void kvm_vfio_ops_exit(void)
 {
 	kvm_unregister_device_ops(KVM_DEV_TYPE_VFIO);
 }
+
+#ifdef CONFIG_HISI_VIRTCCA_HOST
+/**
+ * cvm_vfio_add_kvm_to_smmu_domain - Bind the confidential
+ * virtual machine to smmu domain
+ * @filp: The handle of file
+ * @kvm: The kvm belone to confidential virtual machine
+ *
+ * Returns:
+ * %-ENXIO if set kvm failed or iommu group is null
+ * %0 if set kvm success
+ */
+int cvm_vfio_add_kvm_to_smmu_domain(struct file *filp, void *kv)
+{
+	struct iommu_group *iommu_group;
+	int ret = 0;
+
+	if (!is_virtcca_cvm_enable())
+		return ret;
+
+	/* Upper-level calling interface has added a kv lock, but the
+	 * cvm_arm_smmu_domain_set_kvm interface also need add this lock.
+	 * Therefore, it is necessary to unlock here and completing the
+	 * acquisition, then add the kv lock before return.
+	 */
+	mutex_unlock(&((struct kvm_vfio *)kv)->lock);
+	iommu_group = cvm_vfio_file_iommu_group(filp);
+	if (!iommu_group) {
+		ret = -ENXIO;
+		goto out_lock;
+	}
+	if (cvm_arm_smmu_domain_set_kvm((void *)iommu_group)) {
+		ret = -ENXIO;
+		goto out_lock;
+	}
+
+out_lock:
+	mutex_lock(&((struct kvm_vfio *)kv)->lock);
+	return ret;
+}
+
+/**
+ * virtcca_arm_smmu_get_kvm - Find the kvm
+ * with vfio devices through SMMU domain
+ * @domain: Smmu domain
+ *
+ * Returns:
+ * %kvm if find the kvm with vfio devices
+ * %NULL if kvm is null
+ */
+struct kvm *virtcca_arm_smmu_get_kvm(struct arm_smmu_domain *domain)
+{
+	int ret = -1;
+	struct kvm *kvm;
+	struct kvm_device *dev;
+	struct kvm_vfio *kv;
+	struct kvm_vfio_file *kvf;
+	struct iommu_group *iommu_group;
+	unsigned long flags;
+	struct arm_smmu_master *master;
+
+	spin_lock_irqsave(&domain->devices_lock, flags);
+	/* Get smmu master from smmu domain */
+	list_for_each_entry(master, &domain->devices, domain_head) {
+		if (master && master->num_streams >= 0) {
+			ret = 0;
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&domain->devices_lock, flags);
+	if (ret)
+		return NULL;
+
+	ret = -1;
+	iommu_group = master->dev->iommu_group;
+	mutex_lock(&kvm_lock);
+	list_for_each_entry(kvm, &vm_list, vm_list) {
+		mutex_lock(&kvm->lock);
+		/* Get kvm vfio device list */
+		list_for_each_entry(dev, &kvm->devices, vm_node) {
+			if (dev->ops && strcmp(dev->ops->name, "kvm-vfio") == 0) {
+				kv = (struct kvm_vfio *)dev->private;
+				mutex_lock(&kv->lock);
+				list_for_each_entry(kvf, &kv->file_list, node) {
+					/* Get iommu_group from vfio file */
+					if (cvm_vfio_file_iommu_group(kvf->file) == iommu_group) {
+						ret = 0;
+						break;
+					}
+				}
+				mutex_unlock(&kv->lock);
+				if (!ret)
+					break;
+			}
+		}
+		mutex_unlock(&kvm->lock);
+		if (!ret)
+			break;
+	}
+	mutex_unlock(&kvm_lock);
+
+	if (ret)
+		return NULL;
+	return kvm;
+}
+EXPORT_SYMBOL_GPL(virtcca_arm_smmu_get_kvm);
+
+/**
+ * find_arm_smmu_domain - Find smmu domain list from kvm vfio file
+ * @kvf: Kvm vfio file
+ * @smmu_domain_group_list: List of smmu domain group
+ */
+void find_arm_smmu_domain(struct kvm_vfio_file *kvf, struct list_head *smmu_domain_group_list)
+{
+	struct iommu_group *iommu_group;
+	int ret = 0;
+	struct arm_smmu_domain *arm_smmu_domain = NULL;
+	struct arm_smmu_domain *arm_smmu_domain_node = NULL;
+
+	iommu_group = cvm_vfio_file_iommu_group(kvf->file);
+	arm_smmu_domain = to_smmu_domain(virtcca_iommu_group_get_domain(iommu_group));
+	list_for_each_entry(arm_smmu_domain_node,
+		smmu_domain_group_list, node) {
+		if (arm_smmu_domain_node == arm_smmu_domain) {
+			ret = -1;
+			break;
+		}
+	}
+	if (!ret)
+		list_add_tail(&arm_smmu_domain->node, smmu_domain_group_list);
+}
+
+/**
+ * kvm_get_arm_smmu_domain - Find kvm vfio file from kvm
+ * @kvm: Kvm handle
+ * @smmu_domain_group_list: List of smmu domain group
+ */
+void kvm_get_arm_smmu_domain(struct kvm *kvm, struct list_head *smmu_domain_group_list)
+{
+	struct kvm_device *dev;
+	struct kvm_vfio *kv;
+	struct kvm_vfio_file *kvf;
+
+	INIT_LIST_HEAD(smmu_domain_group_list);
+
+	list_for_each_entry(dev, &kvm->devices, vm_node) {
+		if (dev->ops && strcmp(dev->ops->name, "kvm-vfio") == 0) {
+			kv = (struct kvm_vfio *)dev->private;
+			mutex_lock(&kv->lock);
+			list_for_each_entry(kvf, &kv->file_list, node) {
+				find_arm_smmu_domain(kvf, smmu_domain_group_list);
+			}
+			mutex_unlock(&kv->lock);
+		}
+	}
+}
+
+/**
+ * virtcca_iommu_group_map_msi_address - Find iommu group from kvm vfio file, map it
+ * @kvm: The handle of kvm
+ * @kvf: Kvm vfio file
+ * @smmu_domain: Smmu domain
+ * @pa: Physical address
+ * @map_size: Mapped size
+ */
+int virtcca_iommu_group_map_msi_address(struct kvm *kvm, struct kvm_vfio_file *kvf,
+	struct arm_smmu_domain *smmu_domain, phys_addr_t pa, unsigned long map_size)
+{
+	unsigned long iova;
+	int ret = 0;
+	struct iommu_group *iommu_group = NULL;
+	struct arm_smmu_domain *arm_smmu_domain = NULL;
+
+	iommu_group = cvm_vfio_file_iommu_group(kvf->file);
+	if (iommu_group && virtcca_iommu_group_get_domain(iommu_group)) {
+		arm_smmu_domain = to_smmu_domain(virtcca_iommu_group_get_domain(iommu_group));
+		if (arm_smmu_domain == smmu_domain) {
+			iova = virtcca_get_iommu_device_msi_addr(iommu_group);
+			if (!iova)
+				return -ENXIO;
+			virtcca_set_dev_msi_addr(iommu_group, iova);
+			ret = cvm_map_unmap_ipa_range(kvm, iova, pa, map_size, true);
+			if (ret)
+				return ret;
+		}
+	}
+	return ret;
+}
+
+/* Get iommu group from specific smmu domain, map it */
+int kvm_get_iommu_group_by_domain(struct kvm *kvm, struct arm_smmu_domain *smmu_domain,
+	phys_addr_t pa, unsigned long map_size)
+{
+	int ret = 0;
+	struct kvm_device *dev;
+	struct kvm_vfio *kv;
+	struct kvm_vfio_file *kvf;
+
+	mutex_lock(&kvm->lock);
+	list_for_each_entry(dev, &kvm->devices, vm_node) {
+		/* Get kvm vfio device list */
+		if (dev->ops && strcmp(dev->ops->name, "kvm-vfio") == 0) {
+			kv = (struct kvm_vfio *)dev->private;
+			mutex_lock(&kv->lock);
+			list_for_each_entry(kvf, &kv->file_list, node) {
+				/* Get iommu_group from vfio file, map it */
+				ret = virtcca_iommu_group_map_msi_address(kvm, kvf, smmu_domain,
+					pa, map_size);
+			}
+			mutex_unlock(&kv->lock);
+			if (ret)
+				break;
+		}
+	}
+	mutex_unlock(&kvm->lock);
+	return ret;
+}
+#endif
-- 
2.34.1

